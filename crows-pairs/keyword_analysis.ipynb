{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrowS-Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metric import read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from pororo import Pororo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "crows_data = read_data(\"data/crows_pairs_anonymized.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datatypes\n",
    "- crows_data : (df) pandas dataframe of crows pairs data\n",
    "- crows_sent : (dict) dictionary of sentences with keys of bias type\n",
    "- crows_word : (dict) dictionary of words by its count\n",
    "- crows_word_list : (list) list of words ordered by its count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias types\n",
    "1. race-color\n",
    "2. gender\n",
    "3. sexual-orientation\n",
    "4. religion\n",
    "5. age\n",
    "6. nationality\n",
    "7. disability\n",
    "8. physical-appearance\n",
    "9. socioeconomic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "1. Names\n",
    "    - name total\n",
    "    - name gender\n",
    "    - name race\n",
    "   \n",
    "2. Occupations\n",
    "3. Frequently used adjectives, nouns, and verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crows_sent = {}\n",
    "for _ , data in crows_data.iterrows():\n",
    "    bias = data['bias_type']\n",
    "    if bias not in crows_sent.keys():\n",
    "        crows_sent[bias] = list()\n",
    "#         print(f\"{bias}\")\n",
    "    crows_sent[bias].append(data['sent1'])\n",
    "    crows_sent[bias].append(data['sent2'])\n",
    "len(crows_sent['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param : (str) bias type\n",
    "# return : (list) of words in certain bais type in descending order\n",
    "def get_word_list(df_data, bias_type):\n",
    "    counts = dict()\n",
    "    for index, data in df_data.iterrows():\n",
    "        if data['bias_type'] == bias_type:\n",
    "            sent1 = data['sent1']\n",
    "            sent2 = data['sent2']\n",
    "            sent1 = sent1.strip().split()\n",
    "            sent2 = sent2.strip().split()\n",
    "            word_set = set()\n",
    "            for word1, word2 in zip(sent1, sent2):\n",
    "                word_set.add(word1)\n",
    "                word_set.add(word2)\n",
    "            for word in word_set:\n",
    "                counts[word] = counts.get(word,0)+1\n",
    "    word_list = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(f\"{bias_type} word count : {len(word_list)}\")\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param : (list) word list to remove stopwords\n",
    "# return : (list) word list with stopwords removed\n",
    "def remove_stopwords(word_list):\n",
    "    sw = stopwords.words('english')\n",
    "    sw.append(\"The\")\n",
    "    sw.append(\"I\")\n",
    "    sw.append(\"A\")\n",
    "    no_stopwords = []\n",
    "    for word in word_list:\n",
    "        if word[0] not in sw:\n",
    "            no_stopwords.append(word)\n",
    "    return no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/boychaboy/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('white', 207), ('black', 170), ('people', 154), ('He', 150), ('man', 118), ('like', 96), ('would', 95), ('She', 93), ('White', 77), ('American', 77), ('always', 71), ('poor', 67), ('rich', 47), ('could', 47), ('Americans', 47), ('get', 45), ('men', 44), ('young', 41), ('got', 40), ('old', 40)]\n",
      "Word count : 5043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "crows_word = {}\n",
    "crows_word = dict()\n",
    "for index, data in crows_data.iterrows():\n",
    "    sent1 = data['sent1']\n",
    "    sent2 = data['sent2']\n",
    "    sent1 = sent1.strip().split()\n",
    "    sent2 = sent2.strip().split()\n",
    "    word_set = set()\n",
    "    for word1, word2 in zip(sent1, sent2):\n",
    "        word_set.add(word1)\n",
    "        word_set.add(word2)\n",
    "    for word in word_set:\n",
    "        crows_word[word] = crows_word.get(word,0)+1\n",
    "crows_word = sorted(crows_word.items(), key=lambda x: x[1], reverse=True)\n",
    "crows_word = remove_stopwords(crows_word)\n",
    "\n",
    "print(crows_word[:20])\n",
    "crows_word_list = [word[0].lower() for word in crows_word]\n",
    "print(f\"Word count : {len(crows_word_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_words = remove_stopwords(get_word_list(crows_data, 'gender'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Names\n",
    "### 1.1 Names in all category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param : (list) of words\n",
    "# return : (list) of names in descending order of frequency\n",
    "# def get_names(word_list):\n",
    "ner = Pororo(task=\"ner\", lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_list(sent_list):\n",
    "    name_list = []\n",
    "    for sent in sent_list:\n",
    "        for token in ner(sent):\n",
    "            if token[1] == 'PERSON':\n",
    "                name_list.append(token[0])\n",
    "    return name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict = {}\n",
    "for bias in crows_sent.keys():\n",
    "    for sent in crows_sent[bias]:\n",
    "        for token in ner(sent):\n",
    "            if token[1] == 'PERSON':\n",
    "                if token[0] not in name_dict.keys():\n",
    "                    name_dict[token[0]] = 1\n",
    "                else:\n",
    "                    name_dict[token[0]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort in descending order\n",
    "name_tuple = sorted(name_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "name_list = [name[0] for name in name_tuple]\n",
    "name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def save_dict_to_csv(data, filename):\n",
    "    dict_sorted = sorted(data.items(), key=lambda x:x[1], reverse=True)\n",
    "    df = pd.DataFrame(dict_sorted)\n",
    "    df.to_csv(f\"../data/{filename}.csv\", header=False, index=False)\n",
    "    print(f\"file saved in ../data/{filename}.csv\")\n",
    "    return\n",
    "\n",
    "save_dict_to_csv(name_dict, \"crows_name\")\n",
    "# df = pd.DataFrame(name_list)\n",
    "# df.to_csv(\"../data/crows_name.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Names in each bias categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_data = {}\n",
    "for _, data in crows_data.iterrows():\n",
    "    bias = data['bias_type']\n",
    "    name1 = None; name2 = None\n",
    "    if bias not in name_data.keys():\n",
    "        name_data[bias] = list()\n",
    "    for token in ner(data['sent1']):\n",
    "        if token[1] == 'PERSON':\n",
    "            name1 = token[0]\n",
    "    for token in ner(data['sent2']):\n",
    "        if token[1] == 'PERSON':\n",
    "            name2 = token[0]\n",
    "    if name1 or name2:\n",
    "        name_data[bias].append((name1, name2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of pairs in each bias categories\n",
    "for bias in name_data.keys():\n",
    "    print(f\"{bias} : {len(name_data[bias])} pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_data['gender']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Occupations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Winogender Occupation\n",
    "- crows_wino_occupation : (list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/winogender-schemas/data/occupations-stats.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e8b51a15a9bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwino_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../data/winogender-schemas/data/occupations-stats.tsv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwino_occupation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwino_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/gomu/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gomu/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gomu/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gomu/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gomu/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gomu/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gomu/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/winogender-schemas/data/occupations-stats.tsv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "wino_dir = \"../data_analysis/winogender-schemas/data/occupations-stats.tsv\"\n",
    "wino_occupation = pd.read_csv(wino_dir, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wino_occupation_list = list(wino_occupation['occupation'])\n",
    "wino_occupation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupation_dict = {}\n",
    "for bias in crows_sent.keys():\n",
    "    for sent in crows_sent[bias]:\n",
    "        for word in sent.strip():\n",
    "            if word in wino_occupation_list:\n",
    "                if token[0] not in name_dict.keys():\n",
    "                    occupation_dict[token[0]] = 1\n",
    "                else:\n",
    "                    occupation_dict[token[0]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crows_occupation = {}\n",
    "for occupation in wino_occupation_list:\n",
    "    if occupation in crows_word_list:\n",
    "#         crows_occupation.append(occupation)\n",
    "        crows_occupation[occupation] = crows_occupation.get(occupation,0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crows_wino_occupation = [word for word in crows_occupation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Occupation Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupation_dir = \"../data_analysis/occupations.csv\"\n",
    "f = open(occupation_dir, 'r')\n",
    "occupation_df = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupation_list = [occupation.lower() for occupation in list(occupation_df['Occupations'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1155"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(occupation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crows_word_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f1afb903f078>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcrows_occupation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moccupation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moccupation_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0moccupation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcrows_word_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#         crows_occupation.append(occupation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mcrows_occupation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moccupation\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrows_occupation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moccupation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'crows_word_list' is not defined"
     ]
    }
   ],
   "source": [
    "crows_occupation = {}\n",
    "for occupation in occupation_list:\n",
    "    if occupation in crows_word_list:\n",
    "#         crows_occupation.append(occupation)\n",
    "        crows_occupation[occupation] = crows_occupation.get(occupation,0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(crows_occupation))\n",
    "crows_occupation = [occ for occ in crows_occupation.keys()]\n",
    "crows_occupation[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "crows_occupation2 = {}\n",
    "for bias in crows_sent.keys():\n",
    "    for sent in crows_sent[bias]:\n",
    "        for occupation in occupation_list:\n",
    "            if occupation in sent:\n",
    "                crows_occupation2[occupation] = crows_occupation2.get(occupation,0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "crows_occupation2 = sorted(crows_occupation2.items(), key=lambda x: x[1], reverse=True)\n",
    "crows_occupation_total = [occ[0] for occ in crows_occupation2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('doctor', 42),\n",
       " ('student', 20),\n",
       " ('teacher', 19),\n",
       " ('engineer', 15),\n",
       " ('driver', 14),\n",
       " ('actor', 13),\n",
       " ('pilot', 12),\n",
       " ('lawyer', 12),\n",
       " ('police officer', 8),\n",
       " ('scientist', 7),\n",
       " ('maid', 6),\n",
       " ('janitor', 6),\n",
       " ('soldier', 6),\n",
       " ('employee', 5),\n",
       " ('barber', 4),\n",
       " ('cashier', 4),\n",
       " ('farmer', 4),\n",
       " ('sheriff', 4),\n",
       " ('chef', 4),\n",
       " ('artist', 4),\n",
       " ('mechanic', 3),\n",
       " ('landlord', 3),\n",
       " ('nurse', 3),\n",
       " ('executive', 3),\n",
       " ('waiter', 3),\n",
       " ('publican', 2),\n",
       " ('housekeeper', 2),\n",
       " ('astronomer', 2),\n",
       " ('arts', 2),\n",
       " ('magician', 2),\n",
       " ('grocer', 2),\n",
       " ('medical student', 2),\n",
       " ('airman', 2),\n",
       " ('radiologist', 2),\n",
       " ('foster parent', 2),\n",
       " ('employment', 2),\n",
       " ('salesman', 2),\n",
       " ('builder', 2),\n",
       " ('physicist', 2),\n",
       " ('inspector', 2),\n",
       " ('lawn mower', 2),\n",
       " ('agent', 2),\n",
       " ('aeronautical engineer', 2),\n",
       " ('software engineer', 2),\n",
       " ('supervisor', 2),\n",
       " ('fireman', 2),\n",
       " ('security guard', 2),\n",
       " ('groom', 2),\n",
       " ('butcher', 2),\n",
       " ('dealer', 2),\n",
       " ('plumber', 2),\n",
       " ('postman', 2),\n",
       " ('lifeguard', 2),\n",
       " ('priest', 2),\n",
       " ('instructor', 2),\n",
       " ('butler', 1),\n",
       " ('office worker', 1),\n",
       " ('hostess', 1),\n",
       " ('accountant', 1),\n",
       " ('secretary', 1),\n",
       " ('surgeon', 1),\n",
       " ('cleric', 1),\n",
       " ('gambler', 1),\n",
       " ('hairdresser', 1),\n",
       " ('administrator', 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crows_occupation2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(crows_occupation_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(crows_occupation_total)\n",
    "df.to_csv(\"../data/crows_occupation.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crows_occupation_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Frequently used adjectives, nouns, and verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = SequenceTagger.load(\"flair/pos-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = Sentence(crows_sent['gender'][0])\n",
    "tagger.predict(sentence)\n",
    "sentence.to_tagged_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for entity in sentence.get_spans('pos'):\n",
    "#     print(entity)\n",
    "sentence.get_spans('pos')[1].tokens[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = dict() #noun\n",
    "VB = dict() #verb\n",
    "JJ = dict() #adjective\n",
    "\n",
    "for bias in crows_sent.keys():\n",
    "    for sent in crows_sent[bias]:\n",
    "        sentence = Sentence(sent)\n",
    "        tagger.predict(sentence)\n",
    "        for word in sentence.get_spans('pos'):\n",
    "            tok = word.tokens[0].text\n",
    "            pos = word.tag\n",
    "            if pos.startswith('NN'):\n",
    "                print(f\"{tok}, {pos}\")\n",
    "                NN[tok] = NN.get(tok,0)+1\n",
    "            elif pos.startswith('VB'):\n",
    "                print(f\"{tok}, {pos}\")\n",
    "                VB[tok] = VB.get(tok,0)+1\n",
    "            elif pos.startswith('JJ'):\n",
    "                print(f\"{tok}, {pos}\")\n",
    "                JJ[tok] = JJ.get(tok,0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_to_csv(NN, \"crows_noun\")\n",
    "save_dict_to_csv(VB, \"crows_verb\")\n",
    "save_dict_to_csv(JJ, \"crows_adj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_json(NN, \"crows_noun\")\n",
    "save_to_json(VB, \"crows_noun\")\n",
    "save_to_json(JJ, \"crows_noun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_to_json(data, filename):\n",
    "    json.dump(data, open(f\"../data/json/{filename}.json\", 'w'))\n",
    "    print(f\"file saved in ../data/json/{filename}.json\")\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_json(crows_sent, \"crows_sent\")\n",
    "save_to_json(crows_word, \"crows_word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
