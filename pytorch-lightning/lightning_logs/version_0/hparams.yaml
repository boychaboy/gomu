accumulate_grad_batches: 1
adafactor: false
adam_epsilon: 1.0e-08
attention_dropout: null
cache_dir: ''
config_name: ''
data_dir: ./glue_data/RTE
decoder_layerdrop: null
do_predict: true
do_train: true
dropout: null
encoder_layerdrop: null
eval_batch_size: 32
fp16: false
fp16_opt_level: O2
glue_output_mode: classification
gpus: 1
gradient_clip_val: 1.0
learning_rate: 2.0e-05
lr_scheduler: linear
max_epochs: 3
max_seq_length: 128
model_name_or_path: bert-base-cased
num_workers: 4
output_dir: /home/boychaboy/gomu/transformers/examples/legacy/pytorch-lightning/rte-bert
overwrite_cache: false
seed: 2
task: rte
tokenizer_name: null
tpu_cores: null
train_batch_size: 32
warmup_steps: 0
weight_decay: 0.0
